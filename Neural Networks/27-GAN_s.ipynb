{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"26-GAN's.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPmA9Q+Wt1LDaquV45dlqOq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"Dz1AOjgyEmP1","executionInfo":{"status":"ok","timestamp":1650745102779,"user_tz":300,"elapsed":7,"user":{"displayName":"Shivali Dalmia","userId":"13522911372350483594"}}},"outputs":[],"source":["import tensorflow as tf\n","\n","# Imports\n","from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout, \\\n","  BatchNormalization\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import SGD, Adam\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import sys, os"]},{"cell_type":"code","source":["# Load the data\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()"],"metadata":{"id":"doRaqxZcEtj5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# map inputs to (-1, +1) for better training\n","x_train, x_test = x_train / 255.0 * 2 - 1, x_test / 255.0 * 2 - 1\n","print(\"x_train.shape:\", x_train.shape)"],"metadata":{"id":"lTU5pxjNE55O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Flatten the data\n","N, H, W = x_train.shape\n","D = H * W\n","x_train = x_train.reshape(-1, D)\n","x_test = x_test.reshape(-1, D)\n","print(\"x_train.shape:\", x_train.shape)\n","print(\"x_test.shape:\", x_test.shape)"],"metadata":{"id":"FAH8JFqeE7_Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dimensionality of the latent space\n","latent_dim = 100"],"metadata":{"id":"JepSZgyKE9lh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the generator model\n","def build_generator(latent_dim):\n","  i = Input(shape=(latent_dim,))\n","  x = Dense(256, activation=LeakyReLU(alpha=0.2))(i)\n","  x = BatchNormalization(momentum=0.7)(x)\n","  x = Dense(512, activation=LeakyReLU(alpha=0.2))(x)\n","  x = BatchNormalization(momentum=0.7)(x)\n","  x = Dense(1024, activation=LeakyReLU(alpha=0.2))(x)\n","  x = BatchNormalization(momentum=0.7)(x)\n","  x = Dense(D, activation='tanh')(x)\n","\n","  model = Model(i, x)\n","  return model"],"metadata":{"id":"L9CcdO7zE_rV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the discriminator model\n","def build_discriminator(img_size):\n","  i = Input(shape=(img_size,))\n","  x = Dense(512, activation=LeakyReLU(alpha=0.2))(i)\n","  x = Dense(256, activation=LeakyReLU(alpha=0.2))(x)\n","  x = Dense(1, activation='sigmoid')(x)\n","  model = Model(i, x)\n","  return model"],"metadata":{"id":"T0ICqL-qFBsg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compile both models in preparation for training\n","\n","\n","# Build and compile the discriminator\n","discriminator = build_discriminator(D)\n","discriminator.compile(\n","    loss='binary_crossentropy',\n","    optimizer=Adam(0.0002, 0.5),\n","    metrics=['accuracy'])\n","\n","# Build the generator\n","generator = build_generator(latent_dim)\n"],"metadata":{"id":"OtqCBrHZFGYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create an input to represent noise sample from latent space\n","z = Input(shape=(latent_dim,))\n","\n","# Pass noise through generator to get an image\n","img = generator(z)\n","\n","# Make sure only the generator is trained\n","discriminator.trainable = False\n","\n","# Pass the output of the generator to the discriminator\n","fake_pred = discriminator(img)"],"metadata":{"id":"CKxZWwQYFKZU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the combined model object\n","combined_model = Model(z, fake_pred)\n","\n","# Compile the combined model\n","combined_model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n"],"metadata":{"id":"kGHhfm6_FMc7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the GAN\n","\n","\n","# Config\n","batch_size = 32\n","epochs = 30000\n","sample_period = 200 # every `sample_period` steps generate and save some data"],"metadata":{"id":"s-8UQhL9FOZ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create batch labels to use when calling train_on_batch\n","ones = np.ones(batch_size)\n","zeros = np.zeros(batch_size)"],"metadata":{"id":"mKKnvub_FQZ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Store the losses\n","d_losses = []\n","g_losses = []"],"metadata":{"id":"F9JF4BoyFR_b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a folder to store generated images\n","if not os.path.exists('gan_images'):\n","  os.makedirs('gan_images')"],"metadata":{"id":"zERciF0TFUk1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A function to generate a grid of random samples from the generator\n","# and save them to a file\n","def sample_images(epoch):\n","  rows, cols = 5, 5\n","  noise = np.random.randn(rows * cols, latent_dim)\n","  imgs = generator.predict(noise)\n","\n","  # Rescale images 0 - 1\n","  imgs = 0.5 * imgs + 0.5\n","\n","  fig, axs = plt.subplots(rows, cols)\n","  idx = 0\n","  for i in range(rows):\n","    for j in range(cols):\n","      axs[i,j].imshow(imgs[idx].reshape(H, W), cmap='gray')\n","      axs[i,j].axis('off')\n","      idx += 1\n","  fig.savefig(\"gan_images/%d.png\" % epoch)\n","  plt.close()"],"metadata":{"id":"AZDuItWBFXTr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Main training loop\n","for epoch in range(epochs):\n","  ###########################\n","  ### Train discriminator ###\n","  ###########################\n","  \n","  # Select a random batch of images\n","  idx = np.random.randint(0, x_train.shape[0], batch_size)\n","  real_imgs = x_train[idx]\n","  \n","  # Generate fake images\n","  noise = np.random.randn(batch_size, latent_dim)\n","  fake_imgs = generator.predict(noise)\n","  \n","  # Train the discriminator\n","  # both loss and accuracy are returned\n","  d_loss_real, d_acc_real = discriminator.train_on_batch(real_imgs, ones)\n","  d_loss_fake, d_acc_fake = discriminator.train_on_batch(fake_imgs, zeros)\n","  d_loss = 0.5 * (d_loss_real + d_loss_fake)\n","  d_acc  = 0.5 * (d_acc_real + d_acc_fake)\n","  \n","  \n","  #######################\n","  ### Train generator ###\n","  #######################\n","  \n","  noise = np.random.randn(batch_size, latent_dim)\n","  g_loss = combined_model.train_on_batch(noise, ones)\n","  \n","  # do it again!\n","  noise = np.random.randn(batch_size, latent_dim)\n","  g_loss = combined_model.train_on_batch(noise, ones)\n","  \n","  # Save the losses\n","  d_losses.append(d_loss)\n","  g_losses.append(g_loss)\n","  \n","  if epoch % 100 == 0:\n","    print(f\"epoch: {epoch+1}/{epochs}, d_loss: {d_loss:.2f}, \\\n","      d_acc: {d_acc:.2f}, g_loss: {g_loss:.2f}\")\n","  \n","  if epoch % sample_period == 0:\n","    sample_images(epoch)"],"metadata":{"id":"mCyXjPV6Fakx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(g_losses, label='g_losses')\n","plt.plot(d_losses, label='d_losses')\n","plt.legend()"],"metadata":{"id":"02NaFYn4Fcby"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls gan_images\n","\n","from skimage.io import imread\n","a = imread('gan_images/0.png')\n","plt.imshow(a)\n","\n","a = imread('gan_images/1000.png')\n","plt.imshow(a)\n","\n","a = imread('gan_images/5000.png')\n","plt.imshow(a)\n","\n","a = imread('gan_images/10000.png')\n","plt.imshow(a)\n","\n","a = imread('gan_images/20000.png')\n","plt.imshow(a)\n","\n","a = imread('gan_images/29800.png')\n","plt.imshow(a)"],"metadata":{"id":"ZNfXkOgpFfXq"},"execution_count":null,"outputs":[]}]}